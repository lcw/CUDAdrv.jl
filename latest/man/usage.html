<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Usage · CUDAdrv.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>CUDAdrv.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="../search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li class="current"><a class="toctext" href="usage.html">Usage</a><ul class="internal"><li><a class="toctext" href="#Automatic-memory-management-1">Automatic memory management</a></li><li><a class="toctext" href="#Arrays-1">Arrays</a></li><li><a class="toctext" href="#Modules-and-custom-kernels-1">Modules and custom kernels</a></li><li class="toplevel"><a class="toctext" href="#Other-notes-1">Other notes</a></li><li><a class="toctext" href="#Notes-on-memory-1">Notes on memory</a></li><li><a class="toctext" href="#Troubleshooting-1">Troubleshooting</a></li></ul></li></ul></li><li><span class="toctext">Library</span><ul><li><a class="toctext" href="../lib/api.html">API wrappers</a></li><li><a class="toctext" href="../lib/array.html">Arrays</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li><a href="usage.html">Usage</a></li></ul><a class="edit-page" href="https://github.com/JuliaGPU/CUDAdrv.jl/tree/63ea6662b4f54989041f7e93e22387a2a8a47f1b/docs/src/man/usage.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Usage</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Usage-1" href="#Usage-1">Usage</a></h1><p>Quick start:</p><pre><code class="language-julia">dev = CuDevice(0)
ctx = CuContext(dev)

# Code that does GPU computations

destroy!(ctx)

# output
</code></pre><h2><a class="nav-anchor" id="Automatic-memory-management-1" href="#Automatic-memory-management-1">Automatic memory management</a></h2><p>Except for the encapsulating context, <code>destroy</code> or <code>unload</code> calls are never needed. Objects are registered with the Julia garbage collector, and are automatically finalized when they go out of scope.</p><p>However, many CUDA API functions implicitly depend on global state, such as the current active context. The wrapper needs to model those dependencies in order for objects not to get destroyed before any dependent object is. If we fail to model these dependency relations, API calls might randomly fail, eg. in the case of a missing context dependency with a <code>INVALID_CONTEXT</code> or <code>CONTEXT_IS_DESTROYED</code> error message.</p><p>If this seems to be the case, re-run with <code>TRACE=1</code> and file a bug report.</p><h2><a class="nav-anchor" id="Arrays-1" href="#Arrays-1">Arrays</a></h2><h3><a class="nav-anchor" id="Device-arrays-1" href="#Device-arrays-1">Device arrays</a></h3><p>Device arrays are called <code>CuArray</code>s, as opposed to regular (CPU) Julia <code>Array</code>s</p><p><code>CuArray</code>s can be initialized with regular <code>Array</code>s:</p><pre><code class="language-julia">A   = zeros(Float32,3,4)
d_A = CuArray(A)</code></pre><p>The <code>d_</code> syntax is a conventional way of reminding yourself that the array is allocated on the device.</p><p>To copy a device array back to the host, use</p><pre><code class="language-julia">copy!(A, d_A)</code></pre><p>You can also invert <code>d_A</code> and <code>A</code> to copy from host to device.</p><p>Most of the typical Julia functions, like <code>size</code>, <code>ndims</code>, <code>eltype</code>, etc.,  work on CuArrays. One noteworthy omission is that you can&#39;t directly index a CuArray: <code>d_A[2,4]</code> will fail. This is not supported because host/device memory transfers are relatively slow, and you don&#39;t want to write code that (on the host side) makes use of individual elements in a device array. If you want to inspect the values in a device array, first use <code>copy!</code> to copy it to host memory.</p><h2><a class="nav-anchor" id="Modules-and-custom-kernels-1" href="#Modules-and-custom-kernels-1">Modules and custom kernels</a></h2><p>This will not teach you about CUDA programming; for that, please refer to the CUDA documentation and other online sources.</p><h3><a class="nav-anchor" id="Compiling-your-own-modules-1" href="#Compiling-your-own-modules-1">Compiling your own modules</a></h3><p>You can write and use your own custom kernels, first writing a <code>.cu</code> file and compiling it as a <code>ptx</code> module. On Linux, compilation would look something like this:</p><pre><code class="language-none">nvcc -ptx mycudamodule.cu</code></pre><p>You can specify that the code should be compiled for compute capability 2.0 devices or higher using:</p><pre><code class="language-none">nvcc -ptx -gencode=arch=compute_20,code=sm_20 mycudamodule.cu</code></pre><p>If you want to write code that will support multiple datatypes (e.g., <code>Float32</code> and <code>Float64</code>), it&#39;s recommended that you use C++ and write your code using templates. Then use <code>extern C</code> to instantiate bindings for each datatype. For example:</p><pre><code class="language-cpp">template &lt;typename T&gt;
__device__ void kernel_function1(T *data) {
    // Code goes here
}
template &lt;typename T1, typename T2&gt;
__device__ void kernel_function2(T1 *data1, T2 *data2) {
    // Code goes here
}

extern &quot;C&quot;
{
    void __global__ kernel_function1_float(float *data) {kernel_function1(data);}
    void __global__ kernel_function1_double(double *data) {kernel_function1(data);}
    void __global__ kernel_function2_int_float(int *data1, float *data2) {kernel_function2(data1,data2);}
}</code></pre><h4><a class="nav-anchor" id="Initializing-and-freeing-PTX-modules-1" href="#Initializing-and-freeing-PTX-modules-1">Initializing and freeing PTX modules</a></h4><p>To easily make your kernels available, the recommended approach is to define something analogous to the following for each <code>ptx</code> module (this example uses the kernels described in the previous section):</p><pre><code class="language-julia">module MyCudaModule

import CUDAdrv
import CUDAdrv: CuModule, CuModuleFile, CuFunction, cudacall

export function1

const ptxdict = Dict()
const mdlist = Array{CuModule}(0)

function mdinit(devlist)
    global ptxdict
    global mdlist
    isempty(mdlist) || error(&quot;mdlist is not empty&quot;)
    for dev in devlist
        CuDevice(dev)
        md = CuModuleFile(&quot;mycudamodule.ptx&quot;)

        ptxdict[(&quot;function1&quot;, Float32)] = CuFunction(md, &quot;kernel_function1_float&quot;)
        ptxdict[(&quot;function1&quot;, Float64)] = CuFunction(md, &quot;kernel_function1_double&quot;)
        ptxdict[(&quot;function2&quot;, Int32, Float32)] = CuFunction(md, &quot;kernel_function2_int_float&quot;)

        push!(mdlist, md)
    end
end

mdclose() = (empty!(mdlist); empty!(ptxdict))

function finit()
  mdclose()
end

function init(devlist)
  mdinit(devlist)
end

function function1{T}(griddim::CuDim, blockdim::CuDim, data::CuArray{T})
    cufunction1 = ptxdict[(&quot;function1&quot;, T)]
    cudacall(cufunction1, griddim, blockdim, (Ptr{T},), data)
end

...

end  # MyCudaModule</code></pre><p>Usage will look something like the following:</p><pre><code class="language-julia">gpuid = 0
dev = CuDevice(gpuid) # Or the ID of the GPU you want, if you have many of them
ctx = CuContext(dev)

MyCudaModule.init(gpuid)
# Code that uses functions from your MyCudaModule
MyCudaModule.finit()

destroy!(ctx)</code></pre><h1><a class="nav-anchor" id="Other-notes-1" href="#Other-notes-1">Other notes</a></h1><h2><a class="nav-anchor" id="Notes-on-memory-1" href="#Notes-on-memory-1">Notes on memory</a></h2><p>Julia convention is that matrices are stored in column-major order, whereas C (and CUDA) use row-major. For efficiency this wrapper avoids reordering memory, so that the linear sequence of addresses is the same between main memory and the GPU. For most usages, this is probably what you want.</p><p>However, for the purposes of linear algebra, this effectively means that one is storing the transpose of matrices on the GPU. Keep this in mind when manipulating code on your GPU kernels.</p><h2><a class="nav-anchor" id="Troubleshooting-1" href="#Troubleshooting-1">Troubleshooting</a></h2><p>You can enable verbose logging using two environment variables:</p><ul><li><p><code>DEBUG</code>: if set, enable additional (possibly costly) run-time checks, and some more verbose output</p></li><li><p><code>TRACE</code>: if set, the <code>DEBUG</code> level will be activated, in addition with a trace of every call to the underlying library</p></li></ul><p>In order to avoid run-time cost for checking the log level, these flags are implemented by means of global constants. As a result, you <strong>need to run Julia with precompilation disabled</strong> if you want to modify these flags:</p><pre><code class="language-none">$ TRACE=1 julia --compilecache=no examples/vadd.jl
TRACE: CUDAdrv.jl is running in trace mode, this will generate a lot of additional output
...</code></pre><p>Enabling colors with <code>--color=yes</code> is also recommended as it color-codes the output.</p><footer><hr/><a class="previous" href="../index.html"><span class="direction">Previous</span><span class="title">Home</span></a><a class="next" href="../lib/api.html"><span class="direction">Next</span><span class="title">API wrappers</span></a></footer></article></body></html>
